{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b14463-2c4d-4a53-a8d1-def39d052cff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LAB 07 - Random Forest for Regression\n",
    "\n",
    "In this lab we will be extending the previous lab about Decision trees and build a Regression model using Random Forest.\n",
    "\n",
    "For simplicity, we will be using the same dataset as the previous lab (you can find it in ECLASS).\n",
    "\n",
    "**IMPORTANT:** For this lab, if you haven't finished your code from last week's lab on Decision trees, you will have the option to use the sklearn implementation for a regression tree. However, this doesn't mean that you should skip the previous lab. This is just so that you don't get behind with the content and you don't spend all your time today working on the previous lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3323dc4-95cd-4564-8ccd-441019188fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29dfe0-10af-4871-ab66-e7cffe0685cc",
   "metadata": {},
   "source": [
    "As mentioned before, use the Boston Housing data and prepare your train/val/test split as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6c793-4f97-49ff-a1f3-3e5ec2aa086b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 1 -- Bootstrap\n",
    "\n",
    "Also known as [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating), this technique consists of making several samples with replacement of the original data, using each of the samples to train an estimator, and then aggregating the predictions using the average (this is also a type of model ensemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7aaba32-4c00-404c-9899-7a5759059598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X, num_bags=10):\n",
    "    \"\"\"\n",
    "    Given a dataset and a number of bags,\n",
    "    sample the dataset with replacement.\n",
    "    \n",
    "    This function does not return a copy\n",
    "    of the datapoints, but a list of indices\n",
    "    with compatible dimensionality\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        A dataset\n",
    "    num_bags : int, default 10\n",
    "        The number of bags to create\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of ndarray\n",
    "        The list contains `num_bags` integer one-dimensional ndarrays.\n",
    "        Each of these contains the indices corresponding to the \n",
    "        sampled datapoints in `X`\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    * The number of datapoints in each bach will\n",
    "      match the number of datapoints in the given\n",
    "      dataset.\n",
    "    * The\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(0) # you can change the seed, or use 0 to replicate my results\n",
    "    # lista em que os elementos s√£o arrays com os indices dos elementos que \n",
    "    list_bags = []\n",
    "    n = X.shape[0]\n",
    "    for i in range(num_bags):\n",
    "        rand_idx = np.random.randint(0,n,size=n)\n",
    "        list_bags.append(rand_idx)\n",
    "\n",
    "    return list_bags\n",
    "        \n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "627521aa-2948-485a-917a-17691d853a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29, 32, 26, 73, 23, 69, 53, 79, 19, 95, 41, 33,  4, 70, 64, 54,  9,\n",
       "        1,  7, 62, 94, 29, 40, 38, 29, 46, 66, 86, 47, 74, 21, 68, 26,  8,\n",
       "       94, 84, 27,  5, 58, 83, 79, 21, 16, 89, 89, 47, 98, 95, 48, 59, 75,\n",
       "       39, 21,  2, 68,  5,  2, 85, 86, 17, 13, 78, 27, 60,  1, 44, 52, 26,\n",
       "       22, 36, 14, 82, 68, 25, 80, 76, 90, 14, 46, 75, 47, 72, 65, 21, 37,\n",
       "       84, 46,  1, 39, 71, 79, 29, 66, 78, 34, 15, 57, 71, 17, 43])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "X_small = rng.random(size=(100,2))\n",
    "bags = bootstrap(X_small)\n",
    "bags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff32d32b-f615-43da-b70e-aa8959d01093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([64, 13, 34, 84, 24, 98,  1, 87, 16, 87, 21, 70, 43, 41, 48, 42, 89,\n",
       "       52, 86, 19, 78, 37, 56,  7, 75, 28, 23,  3, 57, 17, 37, 69, 73, 69,\n",
       "       80, 94, 12, 33, 15, 43, 60,  0, 92, 71,  3, 63, 14, 75,  6, 47,  1,\n",
       "       67,  2, 79, 55,  7, 13, 11, 43, 74, 10, 19, 26, 32, 93, 88, 47, 21,\n",
       "       43, 28, 39, 87, 24, 66, 77, 83, 93, 63, 42, 11, 57, 83, 88, 40, 44,\n",
       "       31, 99, 34, 84, 58, 88, 56, 41, 77, 58, 53, 69, 71, 74, 85])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "X_small = rng.random(size=(100,2))\n",
    "bags = bootstrap(X_small)\n",
    "bags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b5126-5aad-4c74-a7de-ad3935acd7e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 2 -- Aggregation\n",
    "\n",
    "The second part of bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baac85e0-2a36-4ee0-92a5-e31a01a70928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_regression(preds):\n",
    "    \"\"\"\n",
    "    Aggregate predictions by several estimators\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : list of ndarray\n",
    "        Predictions from multiple estimators.\n",
    "        All ndarrays in this list should have the same\n",
    "        dimensionality.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    ndarray\n",
    "        The mean of the predictions\n",
    "    \"\"\"\n",
    "    preds = np.array(preds).T\n",
    "\n",
    "    return np.sum(preds,axis=1)/preds.shape[1]\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91f57a-6395-4285-ac01-e1099af84dde",
   "metadata": {},
   "source": [
    "## Exercise 3 -- Random Forest for regression\n",
    "\n",
    "Using the functions you implemented above, it is now time to put all of them together to train several decision trees and then ensemble them to output a single prediction. For the random forest, however, we need to select a subset of features at each split on the decision tree. \n",
    "\n",
    "For this part, you can use the sklearn implementation of Random forest for regression as your estimator for each set of features and bags. See below an example of how to do this, and always remember to check the necessary documentation when using an external function.\n",
    "\n",
    "Some parameters you will have to set are: \n",
    "* num_features: number of features per estimator\n",
    "* min_samples: min number of samples per leaf node\n",
    "* max_depth: maximum depth of the decision tree (each estimator)\n",
    "* num_estimators: number of decision trees you will create using each bag and random set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bb5cb00-bc50-4859-8c60-23068392e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_criterion(region):\n",
    "    \"\"\"\n",
    "    Implements the sum of squared error criterion in a region\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    region : ndarray\n",
    "        Array of shape (N,) containing the values of the target values \n",
    "        for N datapoints in the training set.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The sum of squared error\n",
    "        \n",
    "    Note\n",
    "    ----\n",
    "    The error for an empty region should be infinity (use: float(\"inf\"))\n",
    "    This avoids creating empty regions\n",
    "    \"\"\"\n",
    "    if len(region) < 1:\n",
    "        return float(\"inf\")\n",
    "    mean = np.mean(region)\n",
    "    target = region.reshape(-1,1)\n",
    "    sse = np.sum(np.square(target-mean))\n",
    "    \n",
    "    return sse\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0064c51-dede-41e0-b9f6-07850cf873d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_region(region, feature_index, tau):\n",
    "    \"\"\"\n",
    "    Given a region, splits it based on the feature indicated by\n",
    "    `feature_index`, the region will be split in two, where\n",
    "    one side will contain all points with the feature with values \n",
    "    lower than `tau`, and the other split will contain the \n",
    "    remaining datapoints.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    region : array of size (n_samples, n_features)\n",
    "        a partition of the dataset (or the full dataset) to be split\n",
    "    feature_index : int\n",
    "        the index of the feature (column of the region array) used to make this partition\n",
    "    tau : float\n",
    "        The threshold used to make this partition\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    left_partition : array\n",
    "        indices of the datapoints in `region` where feature < `tau`\n",
    "    right_partition : array\n",
    "        indices of the datapoints in `region` where feature >= `tau` \n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    left_partition = []\n",
    "    right_partition = []\n",
    "    for i in range(region.shape[0]):\n",
    "        if region[i, feature_index] < tau:\n",
    "            left_partition.append(i)\n",
    "        else:\n",
    "            right_partition.append(i)\n",
    "    \n",
    "    return np.array(left_partition), np.array(right_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11eab1ad-23e3-4033-ba72-58b9e9a59cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(X, y):\n",
    "    \"\"\"\n",
    "    Given a dataset (full or partial), splits it on the feature of that minimizes the sum of squared error\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array (n_samples, n_features)\n",
    "        features \n",
    "    y : array (n_samples, )\n",
    "        labels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    decision : dictionary\n",
    "        keys are:\n",
    "        * 'feature_index' -> an integer that indicates the feature (column) of `X` on which the data is split\n",
    "        * 'tau' -> the threshold used to make the split\n",
    "        * 'left_region' -> array of indices where the `feature_index`th feature of X is lower than `tau`\n",
    "        * 'right_region' -> indices not in `low_region`\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    m = X.shape[1]\n",
    "    decision = {'feature_index': 0,\n",
    "                 'tau':0,\n",
    "                 'left_region':0,\n",
    "                 'right_region':0}\n",
    "    lowest_criterion = float(\"inf\")\n",
    "    for j in range(m):\n",
    "        for i in range(n):\n",
    "            tau = X[i,j]\n",
    "            l, r = split_region(X, j, tau)\n",
    "            if len(l) >= 1 and len(r) >= 1:\n",
    "                sum_criterion = regression_criterion(y[l]) + regression_criterion(y[r])\n",
    "            if len(r) < 1: \n",
    "                sum_criterion = regression_criterion(y[l]) + regression_criterion([])\n",
    "            if len(l) < 1:\n",
    "                sum_criterion = regression_criterion(y[r]) + regression_criterion([])\n",
    " \n",
    "            if sum_criterion < lowest_criterion:\n",
    "                lowest_criterion = sum_criterion    \n",
    "                decision['feature_index'] = j\n",
    "                decision['tau'] = tau\n",
    "                decision['left_region'] = l\n",
    "                decision['right_region'] = r\n",
    "            \n",
    "    return decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f075d782-b6d2-42c9-8bc9-1c2205800118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_growth(node, min_samples, max_depth, current_depth, X, y):\n",
    "    \"\"\"\n",
    "    Recursively grows a decision tree.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : dictionary\n",
    "        If the node is terminal, it contains only the \"value\" key, which determines the value to be used as a prediction.\n",
    "        If the node is not terminal, the dictionary has the structure defined by `get_split`\n",
    "    min_samples : int\n",
    "        parameter for stopping criterion if a node has <= min_samples datapoints\n",
    "    max_depth : int\n",
    "        parameter for stopping criterion if a node belongs to this depth\n",
    "    depth : int\n",
    "        current distance from the root\n",
    "    X : array (n_samples, n_features)\n",
    "        features (full dataset)\n",
    "    y : array (n_samples, )\n",
    "        labels (full dataset)\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    To create a terminal node, a dictionary is created with a single \"value\" key, with a value that\n",
    "    is the mean of the target variable\n",
    "    \n",
    "    'left' and 'right' keys are added to non-terminal nodes, which contain (possibly terminal) nodes \n",
    "    from higher levels of the tree:\n",
    "    'left' corresponds to the 'left_region' key, and 'right' to the 'right_region' key\n",
    "    \"\"\"\n",
    "    #verifica se √© node terminal\n",
    "    if 'left_region' not in node or 'right_region' not in node:\n",
    "        return\n",
    "    \n",
    "    #pega os indices do datapoint\n",
    "    left_indices = node['left_region']\n",
    "    right_indices = node['right_region']\n",
    "    \n",
    "    #verifica criterio de parada pro node left\n",
    "    if len(left_indices) <= min_samples or current_depth >= max_depth:\n",
    "        node['left'] = {'value': np.mean(y[left_indices])}\n",
    "    # faz parti√ß√£o do left \n",
    "    else:\n",
    "        node['left'] = get_split(X[left_indices], y[left_indices])\n",
    "        recursive_growth(node['left'], min_samples, max_depth, current_depth + 1, X, y)\n",
    "    \n",
    "    #verifica criterio de parada pro node right\n",
    "    if len(right_indices) <= min_samples or current_depth >= max_depth:\n",
    "        node['right'] = {'value': np.mean(y[right_indices])}\n",
    "    # faz parti√ß√£o do right\n",
    "    else:\n",
    "        node['right'] = get_split(X[right_indices], y[right_indices])\n",
    "        recursive_growth(node['right'], min_samples, max_depth, current_depth + 1, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "728b145b-d64c-4a97-9fdc-d3f521cbe5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(node, sample):\n",
    "    \"\"\"\n",
    "    Makes a prediction based on the decision tree defined by `node`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : dictionary\n",
    "        A node created one of the methods above\n",
    "    sample : array of size (n_features,)\n",
    "        a sample datapoint\n",
    "    \"\"\"\n",
    "    while 'value' not in node:\n",
    "        if sample[node['feature_index']] >= node['tau']:\n",
    "            node = node['right']\n",
    "        else:\n",
    "            node = node['left']\n",
    "    \n",
    "    return node['value']\n",
    "    # your code here\n",
    "        \n",
    "def predict(node, X):\n",
    "    \"\"\"\n",
    "    Makes a prediction based on the decision tree defined by `node`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : dictionary\n",
    "        A node created one of the methods above\n",
    "    X : array of size (n_samples, n_features)\n",
    "        n_samples predictions will be made\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    for x in X:\n",
    "        pred = predict_sample(node,x)\n",
    "        y_pred.append(pred)\n",
    "    \n",
    "    return np.array(y_pred)\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdf5763e-79ba-40d1-8f0d-cc6f6f7c63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.read_csv(\"datasets/BostonHousing.txt\")\n",
    "X = boston.iloc[:,:13].to_numpy()\n",
    "y = boston.iloc[:,13].to_numpy()\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X,y, test_size = 0.2)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp,y_temp, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "48ecbd9d-9628-42d4-88ee-c3c3ea408e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404,)\n",
      "(404,)\n",
      "(404,)\n",
      "(404,)\n",
      "(404,)\n",
      "(404,)\n",
      "(404,)\n",
      "(404,)\n",
      "(404,)\n",
      "(404,)\n"
     ]
    }
   ],
   "source": [
    "## your code goes here:\n",
    "min_samples = 20\n",
    "max_depth = 6\n",
    "list_bags = bootstrap(X_train,num_bags=10)\n",
    "preds = []\n",
    "for bag in list_bags:\n",
    "    root = get_split(X_train[bag],y_train[bag])\n",
    "    recursive_growth(root,min_samples,max_depth,1, X_train[bag], y_train[bag])\n",
    "    y_pred = predict(root,X_train[bag])\n",
    "    print(y_pred.shape)\n",
    "    preds.append(y_pred)\n",
    "final_preds = aggregate_regression(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5a3b9c6-58ce-4d78-a9b4-78f5e1bd6ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e5ef4d4b-69da-492f-b9c1-afd291f3ee80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.288186709361407"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = np.sqrt(np.mean(np.square(y_train-final_preds)))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c4872e-3b63-44b0-a5aa-c6ce16f48420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
